{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db5fa13b-d2da-468b-b92e-544ede039eb9",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">U-Net</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0168fe29-95ab-480f-b03f-f4ef6381e095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional\n",
    "from torch import nn\n",
    "from torch.nn.functional import relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56b6f4e-e532-4703-8784-980331d89639",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConvolution(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Primeira camada convolucional 3x3\n",
    "        self.first = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        # Segunda camada convolucional 3x3\n",
    "        self.second = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Executa as duas primeiras camadas convolucionais\n",
    "        x = self.first(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.second(x)\n",
    "        return self.act2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be8b077-761c-4e81-861c-d89010b3f9e3",
   "metadata": {},
   "source": [
    "## Redu√ßao da amostragem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ea2528-4088-4f58-9e88-e9c415d8975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownSample(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.pool(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96963b3-22f7-406b-9dd0-a1b729bcda66",
   "metadata": {},
   "source": [
    "## Aumento da amostragem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6ba377-59b5-4ba3-9fff-92f86def853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        # Convolucao apliando dimensao\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.up(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b788df4-c7dd-45f0-ba9c-e5bfdf2b8a23",
   "metadata": {},
   "source": [
    "## Crop e Concatenar o mapa de caracteristicas(Feature map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f798b73b-0577-4a0a-9d60-51f2e806f5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropAndConcatenate(nn.Module):\n",
    "    def forward(self, x: torch.Tensor, contracting_x: torch.Tensor):\n",
    "        # Ajusta o tamaho do feature map para o tamanho correto\n",
    "        contracting_x = torchvision.transforms.functional.center_crop(contracting_x, [x.shape[2], x.shape[3]])\n",
    "        # Concatena o feature map\n",
    "        x = torch.cat([x, contracting_x], dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61663b0-835c-47d1-affe-c76622c2ddd3",
   "metadata": {},
   "source": [
    "## U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342ce949-f957-4cfb-8574-cd2b07898f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        down_conv_sizes = [(3, 64), (64, 128), (128, 256), (256, 512)]\n",
    "        self.down_conv = nn.ModuleList([DoubleConvolution(i, o) for i,o in down_conv_sizes])\n",
    "\n",
    "        self.down_sample = nn.ModuleList([DownSample() for _ in range(4)])\n",
    "\n",
    "        self.middle_conv = DoubleConvolution(512, 1024)\n",
    "\n",
    "        upsample_sizes = [(1024, 512), (512, 256), (256, 128), (128, 64)]\n",
    "\n",
    "        self.up_sample = nn.ModuleList([UpSample(i, o) for i, o in upsample_sizes])\n",
    "\n",
    "        up_conv_sizes = [(1024, 512), (512, 256), (256, 128), (128, 64)]\n",
    "        self.up_conv = nn.ModuleList([DoubleConvolution(i, o) for i, o in up_conv_sizes])\n",
    "\n",
    "        self.concat = nn.ModuleList([CropAndConcatenate() for _ in range(4)])\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        pass_trough = []\n",
    "        for i in range(len(self.down_conv)):\n",
    "            x = self.down_conv[i](x)\n",
    "            pass_trough.append(x)\n",
    "            x = self.down_sample[i](x)\n",
    "\n",
    "        x = self.middle_conv(x)\n",
    "\n",
    "        for i in range(len(self.up_conv)):\n",
    "            x = self.up_sample[i](x)\n",
    "            x = self.concat[i](x, pass_trough.pop())\n",
    "            x = self.up_conv[i](x)\n",
    "\n",
    "        out = self.final_conv(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa61d25a-abf4-4d8e-9105-b246ffdde9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29015be2-81cb-487d-b7db-dcf267120b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.run(UNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5de56e-6c48-42ae-9b89-26befe9ea247",
   "metadata": {},
   "source": [
    "## U-Net mais simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2282ab-f890-46e8-b889-43a97947f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetSimpler(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        # In the encoder, convolutional layers with the Conv2d function are used to extract features from the input image. \n",
    "        # Each block in the encoder consists of two convolutional layers followed by a max-pooling layer, with the exception of the last block which does not include a max-pooling layer.\n",
    "        # -------\n",
    "        # input: 572x572x3\n",
    "        self.e11 = nn.Conv2d(3, 64, kernel_size=3, padding=1) # output: 570x570x64\n",
    "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding=1) # output: 568x568x64\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 284x284x64\n",
    "\n",
    "        # input: 284x284x64\n",
    "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # output: 282x282x128\n",
    "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding=1) # output: 280x280x128\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 140x140x128\n",
    "\n",
    "        # input: 140x140x128\n",
    "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding=1) # output: 138x138x256\n",
    "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding=1) # output: 136x136x256\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 68x68x256\n",
    "\n",
    "        # input: 68x68x256\n",
    "        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding=1) # output: 66x66x512\n",
    "        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output: 64x64x512\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 32x32x512\n",
    "\n",
    "        # input: 32x32x512\n",
    "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding=1) # output: 30x30x1024\n",
    "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1) # output: 28x28x1024\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        # In the decoder, transpose convolutional layers with the ConvTranspose2d function are used to upsample the feature maps to the original size of the input image. \n",
    "        # Each block in the decoder consists of an upsampling layer, a concatenation with the corresponding encoder feature map, and two convolutional layers.\n",
    "        # -------\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        xe11 = relu(self.e11(x))\n",
    "        xe12 = relu(self.e12(xe11))\n",
    "        xp1 = self.pool1(xe12)\n",
    "\n",
    "        xe21 = relu(self.e21(xp1))\n",
    "        xe22 = relu(self.e22(xe21))\n",
    "        xp2 = self.pool2(xe22)\n",
    "\n",
    "        xe31 = relu(self.e31(xp2))\n",
    "        xe32 = relu(self.e32(xe31))\n",
    "        xp3 = self.pool3(xe32)\n",
    "\n",
    "        xe41 = relu(self.e41(xp3))\n",
    "        xe42 = relu(self.e42(xe41))\n",
    "        xp4 = self.pool4(xe42)\n",
    "\n",
    "        xe51 = relu(self.e51(xp4))\n",
    "        xe52 = relu(self.e52(xe51))\n",
    "        \n",
    "        # Decoder\n",
    "        xu1 = self.upconv1(xe52)\n",
    "        xu11 = torch.cat([xu1, xe42], dim=1)\n",
    "        xd11 = relu(self.d11(xu11))\n",
    "        xd12 = relu(self.d12(xd11))\n",
    "\n",
    "        xu2 = self.upconv2(xd12)\n",
    "        xu22 = torch.cat([xu2, xe32], dim=1)\n",
    "        xd21 = relu(self.d21(xu22))\n",
    "        xd22 = relu(self.d22(xd21))\n",
    "\n",
    "        xu3 = self.upconv3(xd22)\n",
    "        xu33 = torch.cat([xu3, xe22], dim=1)\n",
    "        xd31 = relu(self.d31(xu33))\n",
    "        xd32 = relu(self.d32(xd31))\n",
    "\n",
    "        xu4 = self.upconv4(xd32)\n",
    "        xu44 = torch.cat([xu4, xe12], dim=1)\n",
    "        xd41 = relu(self.d41(xu44))\n",
    "        xd42 = relu(self.d42(xd41))\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(xd42)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680fe1d7-7992-44bc-a730-81ee46381bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d112b3a-1547-437e-b4ae-af30460ce975",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.run(UNetSimpler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3febacd-3d70-4d62-a917-85c7e49b476e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
